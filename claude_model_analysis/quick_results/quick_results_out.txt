Loading data...
Loaded 1528 samples
Using 34 features

Class distribution:
  Class 0: 1284 (84.0%)
  Class 1: 244 (16.0%)

Computed class weights: {np.int32(0): np.float64(0.5950155763239875), np.int32(1): np.float64(3.1311475409836067)}

Train set: 1222 samples
Test set: 306 samples

================================================================================
MODEL TRAINING AND EVALUATION
================================================================================

Random Forest (balanced)
----------------------------------------
  Accuracy:  0.7941
  Precision: 0.2308
  Recall:    0.1224
  F1 Score:  0.1600
  ROC-AUC:   0.5625
  Balanced Accuracy: 0.0612
  CV F1:     0.1934 (±0.0541)
  Training time: 0.59s

Extra Trees (balanced)
----------------------------------------
  Accuracy:  0.7386
  Precision: 0.2075
  Recall:    0.2245
  F1 Score:  0.2157
  ROC-AUC:   0.5588
  Balanced Accuracy: 0.1122
  CV F1:     0.2613 (±0.0762)
  Training time: 0.26s

Gradient Boosting
----------------------------------------
  Accuracy:  0.7778
  Precision: 0.1200
  Recall:    0.0612
  F1 Score:  0.0811
  ROC-AUC:   0.5236
  Balanced Accuracy: 0.0306
  CV F1:     0.1505 (±0.0383)
  Training time: 1.68s

Random Forest (weighted)
----------------------------------------
  Accuracy:  0.7876
  Precision: 0.2143
  Recall:    0.1224
  F1 Score:  0.1558
  ROC-AUC:   0.5670
  Balanced Accuracy: 0.0612
  CV F1:     0.2004 (±0.0505)
  Training time: 0.45s

Logistic Regression (L2)
----------------------------------------
  Accuracy:  0.5588
  Precision: 0.1884
  Recall:    0.5306
  F1 Score:  0.2781
  ROC-AUC:   0.5895
  Balanced Accuracy: 0.2653
  CV F1:     0.2510 (±0.0345)
  Training time: 0.10s

Logistic Regression (L1)
----------------------------------------
/home/grads/a/ashwin_k/lowRISC/lowrisc_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
/home/grads/a/ashwin_k/lowRISC/lowrisc_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
/home/grads/a/ashwin_k/lowRISC/lowrisc_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
/home/grads/a/ashwin_k/lowRISC/lowrisc_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
/home/grads/a/ashwin_k/lowRISC/lowrisc_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn(
  Accuracy:  0.5882
  Precision: 0.1818
  Recall:    0.4490
  F1 Score:  0.2588
  ROC-AUC:   0.5413
  Balanced Accuracy: 0.2245
  CV F1:     0.2898 (±0.0245)
  Training time: 26.60s

Decision Tree
----------------------------------------
  Accuracy:  0.6078
  Precision: 0.1913
  Recall:    0.4490
  F1 Score:  0.2683
  ROC-AUC:   0.5510
  Balanced Accuracy: 0.2245
  CV F1:     0.2917 (±0.0141)
  Training time: 0.02s

================================================================================
RESULTS SUMMARY (sorted by F1 Score)
================================================================================
                   model       f1  precision   recall  roc_auc  balanced_accuracy  cv_f1_mean
Logistic Regression (L2) 0.278075   0.188406 0.530612 0.589494           0.265306    0.250972
           Decision Tree 0.268293   0.191304 0.448980 0.551020           0.224490    0.291708
Logistic Regression (L1) 0.258824   0.181818 0.448980 0.541293           0.224490    0.289836
  Extra Trees (balanced) 0.215686   0.207547 0.224490 0.558763           0.112245    0.261347
Random Forest (balanced) 0.160000   0.230769 0.122449 0.562535           0.061224    0.193402
Random Forest (weighted) 0.155844   0.214286 0.122449 0.566982           0.061224    0.200403
       Gradient Boosting 0.081081   0.120000 0.061224 0.523624           0.030612    0.150538

================================================================================
BEST MODEL: Logistic Regression (L2)
================================================================================

Classification Report:
               precision    recall  f1-score   support

  No Coverage       0.86      0.56      0.68       257
Adds Coverage       0.19      0.53      0.28        49

     accuracy                           0.56       306
    macro avg       0.53      0.55      0.48       306
 weighted avg       0.76      0.56      0.62       306


Results saved to quick_results/comparison_results.csv
Best model saved to quick_results/best_model.joblib

================================================================================
RECOMMENDATIONS
================================================================================
1. Best overall model: Logistic Regression (L2)
   - F1 Score: 0.2781
   - Best for balanced precision/recall trade-off

2. For minimizing false positives: Random Forest (balanced)
   - Precision: 0.2308

4. Practical deployment suggestions:
   - Use probability thresholds to tune precision/recall trade-off
   - Consider ensemble of top 3 models for robustness
   - Monitor performance on new test patterns

5. Try with --include-coverage to see if coverage deltas improve predictions

================================================================================
